{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ YouTube Clickbait Detector - LightGBM Model v2\n",
    "\n",
    "A streamlined machine learning pipeline using **LightGBM** for YouTube clickbait detection.\n",
    "\n",
    "**Features:**\n",
    "- Advanced text preprocessing with TF-IDF vectorization\n",
    "- Comprehensive feature engineering (36 features)\n",
    "- LightGBM classifier with optimized hyperparameters\n",
    "- Post-training verification\n",
    "- Model persistence for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_recall_curve\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load Dataset\n",
    "\n",
    "Upload your `MASTER_DATASET.csv` file to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "DATASET_PATH = list(uploaded.keys())[0]\n",
    "print(f\"Loaded: {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_TFIDF_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Clickbait indicator keywords\n",
    "CLICKBAIT_KEYWORDS = [\n",
    "    'shocking', 'exposed', 'truth', 'secret', 'viral', 'leaked',\n",
    "    \"you won't believe\", 'must watch', 'watch till end', 'nobody tells',\n",
    "    'miracle', 'guaranteed', 'speechless', 'exclusive', 'breaking',\n",
    "    'urgent', 'warning', 'banned', 'deleted', 'hidden', 'revealed'\n",
    "]\n",
    "\n",
    "PIRACY_KEYWORDS = [\n",
    "    'download', 'telegram', 'camrip', 'dvdrip', 'hdrip', 'torrent',\n",
    "    'leaked', 'bolly4u', 'filmyzilla', 'hdcam', 'pre-dvd', 'webrip'\n",
    "]\n",
    "\n",
    "EMOTIONAL_EMOJIS = ['üò±', 'üî•', '‚ò†Ô∏è', 'üí•', 'ü§Ø', 'üò∂', 'üò≠', 'üò°', 'üíÄ', '‚ö†Ô∏è']\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    \"\"\"Count occurrences of keywords in text.\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    return sum(1 for kw in keywords if kw in text_lower)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text for TF-IDF vectorization.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare the dataset with initial cleaning.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìÇ LOADING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Keep only verified rows\n",
    "    if \"verified\" in df.columns:\n",
    "        df = df[df[\"verified\"] == 1].copy()\n",
    "        print(f\"After filtering verified: {len(df)} rows\")\n",
    "\n",
    "    # Fill missing values\n",
    "    text_cols = [\"title\", \"description\", \"thumbnail_text_cleaned\"]\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"\")\n",
    "\n",
    "    num_cols = [\"duration_min\", \"views\", \"likes\", \"thumbnail_text_valid\"]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    # Create combined text field\n",
    "    df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"] + \" \" + df[\"thumbnail_text_cleaned\"]\n",
    "\n",
    "    print(f\"\\nüìä Class distribution:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "    print(f\"Clickbait ratio: {df['label'].mean()*100:.1f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_and_prepare_data(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract comprehensive text-based features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üî§ FEATURE ENGINEERING - TEXT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Basic length features\n",
    "    df[\"title_length\"] = df[\"title\"].str.len()\n",
    "    df[\"desc_length\"] = df[\"description\"].str.len()\n",
    "    df[\"title_word_count\"] = df[\"title\"].str.split().str.len().fillna(0)\n",
    "    df[\"desc_word_count\"] = df[\"description\"].str.split().str.len().fillna(0)\n",
    "\n",
    "    # Title style features\n",
    "    df[\"caps_ratio\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)\n",
    "    )\n",
    "    df[\"title_caps_words\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for w in str(x).split() if w.isupper() and len(w) > 1)\n",
    "    )\n",
    "\n",
    "    # Punctuation features\n",
    "    df[\"question_count\"] = df[\"title\"].str.count(r\"\\?\")\n",
    "    df[\"exclam_count\"] = df[\"title\"].str.count(r\"!\")\n",
    "    df[\"ellipsis_count\"] = df[\"title\"].str.count(r\"\\.\\.\\.\")\n",
    "    df[\"pipe_count\"] = df[\"title\"].str.count(r\"\\|\")\n",
    "\n",
    "    # Emoji features\n",
    "    df[\"emoji_count\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for c in str(x) if ord(c) > 127462)\n",
    "    )\n",
    "    df[\"emotional_emoji_count\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for e in EMOTIONAL_EMOJIS if e in str(x))\n",
    "    )\n",
    "\n",
    "    # Clickbait keyword detection\n",
    "    df[\"clickbait_keywords\"] = df[\"title\"].apply(lambda x: count_keywords(x, CLICKBAIT_KEYWORDS))\n",
    "    df[\"piracy_keywords\"] = (\n",
    "        df[\"title\"].apply(lambda x: count_keywords(x, PIRACY_KEYWORDS)) +\n",
    "        df[\"description\"].apply(lambda x: count_keywords(x, PIRACY_KEYWORDS))\n",
    "    )\n",
    "\n",
    "    # Description quality indicators\n",
    "    df[\"desc_is_empty\"] = (df[\"desc_length\"] < 20).astype(int)\n",
    "    df[\"desc_hashtag_count\"] = df[\"description\"].str.count(r\"#\")\n",
    "    df[\"desc_hashtag_ratio\"] = df[\"desc_hashtag_count\"] / (df[\"desc_word_count\"] + 1)\n",
    "    df[\"desc_has_links\"] = df[\"description\"].str.contains(r\"http|https|www\\.\", regex=True).astype(int)\n",
    "\n",
    "    # Special patterns\n",
    "    df[\"has_full_movie_claim\"] = df[\"title\"].str.lower().str.contains(\n",
    "        r\"full movie|full hindi movie|full hd movie|complete movie\", regex=True\n",
    "    ).astype(int)\n",
    "    df[\"has_year_in_title\"] = df[\"title\"].str.contains(r\"\\b20[0-2][0-9]\\b\", regex=True).astype(int)\n",
    "    df[\"has_hd_4k\"] = df[\"title\"].str.lower().str.contains(r\"\\bhd\\b|\\b4k\\b|\\b1080p\\b\", regex=True).astype(int)\n",
    "\n",
    "    print(\"‚úÖ Text features extracted successfully!\")\n",
    "    return df\n",
    "\n",
    "df = extract_text_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_engagement_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract engagement and metadata features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìà FEATURE ENGINEERING - ENGAGEMENT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Engagement ratios\n",
    "    df[\"likes_view_ratio\"] = df[\"likes\"] / (df[\"views\"] + 1)\n",
    "    df[\"likes_per_minute\"] = df[\"likes\"] / (df[\"duration_min\"] + 0.1)\n",
    "    df[\"views_per_minute\"] = df[\"views\"] / (df[\"duration_min\"] + 0.1)\n",
    "\n",
    "    # Log-transformed features\n",
    "    df[\"log_views\"] = np.log1p(df[\"views\"])\n",
    "    df[\"log_likes\"] = np.log1p(df[\"likes\"])\n",
    "    df[\"log_duration\"] = np.log1p(df[\"duration_min\"])\n",
    "\n",
    "    # Duration-based features\n",
    "    df[\"is_short_video\"] = (df[\"duration_min\"] < 1).astype(int)\n",
    "    df[\"is_very_long\"] = (df[\"duration_min\"] > 60).astype(int)\n",
    "    df[\"duration_mismatch\"] = (\n",
    "        (df[\"has_full_movie_claim\"] == 1) & (df[\"duration_min\"] < 60)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Anomaly detection features\n",
    "    df[\"engagement_score\"] = (\n",
    "        df[\"likes_view_ratio\"] * 100 +\n",
    "        np.log1p(df[\"views\"]) / 10\n",
    "    )\n",
    "    df[\"low_engagement\"] = (\n",
    "        (df[\"likes_view_ratio\"] < 0.001) & (df[\"views\"] > 10000)\n",
    "    ).astype(int)\n",
    "\n",
    "    print(\"‚úÖ Engagement features extracted successfully!\")\n",
    "    return df\n",
    "\n",
    "df = extract_engagement_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df: pd.DataFrame, max_features: int = 5000) -> Tuple:\n",
    "    \"\"\"Build the complete feature matrix.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üî® BUILDING FEATURE MATRIX\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # IMPORTANT: Clean text for TF-IDF vectorization\n",
    "    df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    # TF-IDF Vectorization (uses cleaned text!)\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    X_text = tfidf.fit_transform(df[\"text_clean\"])\n",
    "    print(f\"TF-IDF features: {X_text.shape[1]}\")\n",
    "\n",
    "    # Category encoding\n",
    "    cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    X_cat = cat_encoder.fit_transform(df[[\"category\"]])\n",
    "    print(f\"Categories: {df['category'].nunique()}\")\n",
    "\n",
    "    # Numerical features (36 total)\n",
    "    num_features = [\n",
    "        \"duration_min\", \"views\", \"likes\", \"thumbnail_text_valid\",\n",
    "        \"title_length\", \"desc_length\", \"title_word_count\", \"desc_word_count\",\n",
    "        \"caps_ratio\", \"title_caps_words\",\n",
    "        \"question_count\", \"exclam_count\", \"ellipsis_count\", \"pipe_count\",\n",
    "        \"emoji_count\", \"emotional_emoji_count\",\n",
    "        \"clickbait_keywords\", \"piracy_keywords\",\n",
    "        \"desc_is_empty\", \"desc_hashtag_count\", \"desc_hashtag_ratio\", \"desc_has_links\",\n",
    "        \"has_full_movie_claim\", \"has_year_in_title\", \"has_hd_4k\",\n",
    "        \"likes_view_ratio\", \"likes_per_minute\", \"views_per_minute\",\n",
    "        \"log_views\", \"log_likes\", \"log_duration\",\n",
    "        \"is_short_video\", \"is_very_long\", \"duration_mismatch\",\n",
    "        \"engagement_score\", \"low_engagement\"\n",
    "    ]\n",
    "\n",
    "    X_num = df[num_features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "    # Combine all features\n",
    "    X = hstack([X_text, csr_matrix(X_num_scaled), csr_matrix(X_cat)])\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    print(f\"\\nüìê Final feature matrix shape: {X.shape}\")\n",
    "    print(f\"  - Text features: {X_text.shape[1]}\")\n",
    "    print(f\"  - Numerical features: {len(num_features)}\")\n",
    "    print(f\"  - Category features: {X_cat.shape[1]}\")\n",
    "\n",
    "    return X, y, tfidf, scaler, cat_encoder, num_features, df\n",
    "\n",
    "X, y, tfidf, scaler, cat_encoder, num_features, df = build_feature_matrix(df, MAX_TFIDF_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X, y) -> Dict:\n",
    "    \"\"\"Train LightGBM model.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ TRAINING LIGHTGBM MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=RANDOM_STATE, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "    # Train LightGBM\n",
    "    lgb_model = LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    print(\"\\nüîÑ Training in progress...\")\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Validation metrics\n",
    "    y_pred_val = lgb_model.predict(X_val)\n",
    "    y_prob_val = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    val_f1 = f1_score(y_val, y_pred_val)\n",
    "    val_auc = roc_auc_score(y_val, y_prob_val)\n",
    "\n",
    "    print(f\"\\n‚úÖ Validation Results:\")\n",
    "    print(f\"   F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"   ROC-AUC: {val_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"model\": lgb_model,\n",
    "        \"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test,\n",
    "        \"y_train\": y_train, \"y_val\": y_val, \"y_test\": y_test,\n",
    "        \"val_f1\": val_f1, \"val_auc\": val_auc\n",
    "    }\n",
    "\n",
    "results = train_lightgbm(X, y)\n",
    "model = results[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_test = results[\"X_test\"]\n",
    "y_test = results[\"y_test\"]\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nüî≤ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=[\"Non-Clickbait\", \"Clickbait\"]))\n",
    "\n",
    "print(f\"üéØ ROC-AUC Score: {roc_auc_score(y_test, y_prob_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Post-Training Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üîç POST-TRAINING VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test on label=0 samples\n",
    "print(\"\\nüìó Testing on NON-CLICKBAIT samples (label=0):\")\n",
    "test_0 = df[df['label'] == 0].head(5)\n",
    "correct_0 = 0\n",
    "for idx, row in test_0.iterrows():\n",
    "    sample_df = pd.DataFrame([row])\n",
    "    X_text_s = tfidf.transform(sample_df[\"text_clean\"])\n",
    "    X_num_s = scaler.transform(sample_df[num_features].values)\n",
    "    X_cat_s = cat_encoder.transform(sample_df[[\"category\"]])\n",
    "    X_s = hstack([X_text_s, csr_matrix(X_num_s), csr_matrix(X_cat_s)])\n",
    "    \n",
    "    pred = model.predict(X_s)[0]\n",
    "    probs = model.predict_proba(X_s)[0]\n",
    "    \n",
    "    status = \"‚úÖ\" if pred == 0 else \"‚ùå\"\n",
    "    if pred == 0:\n",
    "        correct_0 += 1\n",
    "    print(f\"  {status} Pred={pred}, Prob[0]={probs[0]:.3f}, Prob[1]={probs[1]:.3f}\")\n",
    "\n",
    "# Test on label=1 samples\n",
    "print(\"\\nüìï Testing on CLICKBAIT samples (label=1):\")\n",
    "test_1 = df[df['label'] == 1].head(5)\n",
    "correct_1 = 0\n",
    "for idx, row in test_1.iterrows():\n",
    "    sample_df = pd.DataFrame([row])\n",
    "    X_text_s = tfidf.transform(sample_df[\"text_clean\"])\n",
    "    X_num_s = scaler.transform(sample_df[num_features].values)\n",
    "    X_cat_s = cat_encoder.transform(sample_df[[\"category\"]])\n",
    "    X_s = hstack([X_text_s, csr_matrix(X_num_s), csr_matrix(X_cat_s)])\n",
    "    \n",
    "    pred = model.predict(X_s)[0]\n",
    "    probs = model.predict_proba(X_s)[0]\n",
    "    \n",
    "    status = \"‚úÖ\" if pred == 1 else \"‚ùå\"\n",
    "    if pred == 1:\n",
    "        correct_1 += 1\n",
    "    print(f\"  {status} Pred={pred}, Prob[0]={probs[0]:.3f}, Prob[1]={probs[1]:.3f}\")\n",
    "\n",
    "total_correct = correct_0 + correct_1\n",
    "print(f\"\\nüéØ Verification Accuracy: {total_correct}/10 ({total_correct*10}%)\")\n",
    "\n",
    "if total_correct >= 7:\n",
    "    print(\"‚úÖ Model verification PASSED!\")\n",
    "else:\n",
    "    print(\"‚ùå Model verification FAILED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üíæ SAVING MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "joblib.dump(model, \"clickbait_model.joblib\")\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "joblib.dump(cat_encoder, \"cat_encoder.joblib\")\n",
    "joblib.dump(num_features, \"num_features.joblib\")\n",
    "\n",
    "print(\"‚úÖ All model files saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading model files...\")\n",
    "files.download('clickbait_model.joblib')\n",
    "files.download('tfidf_vectorizer.joblib')\n",
    "files.download('scaler.joblib')\n",
    "files.download('cat_encoder.joblib')\n",
    "files.download('num_features.joblib')\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Move the 5 .joblib files to your backend/BEST_FINAL_MODEL/ folder\")\n",
    "print(\"2. Run: python app.py\")\n",
    "print(\"3. Reload your Chrome extension\")\n",
    "print(\"4. Test on YouTube videos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüèÜ Model: LightGBM\")\n",
    "print(f\"üìä Validation F1: {results['val_f1']:.4f}\")\n",
    "print(f\"üìä Validation AUC: {results['val_auc']:.4f}\")\n",
    "print(f\"üîç Verification: {total_correct}/10 correct\")\n",
    "print(\"\\n‚úÖ Model saved and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
