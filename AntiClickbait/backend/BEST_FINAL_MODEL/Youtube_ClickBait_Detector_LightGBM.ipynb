{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ YouTube Clickbait Detector - LightGBM Model\n",
    "\n",
    "A streamlined machine learning pipeline using **LightGBM** for YouTube clickbait detection.\n",
    "\n",
    "**Features:**\n",
    "- Advanced text preprocessing with TF-IDF vectorization\n",
    "- Comprehensive feature engineering (30+ features)\n",
    "- LightGBM classifier with optimized hyperparameters\n",
    "- Model persistence for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "import joblib\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_recall_curve\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Load Dataset\n",
    "\n",
    "Upload your `MASTER_DATASET.csv` file to Colab or mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload file directly\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "DATASET_PATH = list(uploaded.keys())[0]\n",
    "\n",
    "# Option 2: Mount Google Drive (uncomment if needed)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATASET_PATH = '/content/drive/MyDrive/your_path/MASTER_DATASET.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare the dataset with initial cleaning.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìÇ LOADING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Keep only verified rows\n",
    "    df = df[df[\"verified\"] == 1].copy()\n",
    "    \n",
    "    # Fill missing values\n",
    "    text_cols = [\"title\", \"description\", \"thumbnail_text_cleaned\"]\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "    \n",
    "    num_cols = [\"duration_min\", \"views\", \"likes\", \"thumbnail_text_valid\"]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Create combined text field\n",
    "    df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"] + \" \" + df[\"thumbnail_text_cleaned\"]\n",
    "    \n",
    "    print(f\"\\nüìä Class distribution:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "    print(f\"Clickbait ratio: {df['label'].mean()*100:.1f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_and_prepare_data(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clickbait indicator keywords\n",
    "CLICKBAIT_KEYWORDS = [\n",
    "    'shocking', 'exposed', 'truth', 'secret', 'viral', 'leaked',\n",
    "    \"you won't believe\", 'must watch', 'watch till end', 'nobody tells',\n",
    "    'miracle', 'guaranteed', 'speechless', 'exclusive', 'breaking',\n",
    "    'urgent', 'warning', 'banned', 'deleted', 'hidden', 'revealed'\n",
    "]\n",
    "\n",
    "PIRACY_KEYWORDS = [\n",
    "    'download', 'telegram', 'camrip', 'dvdrip', 'hdrip', 'torrent',\n",
    "    'leaked', 'bolly4u', 'filmyzilla', 'hdcam', 'pre-dvd', 'webrip'\n",
    "]\n",
    "\n",
    "EMOTIONAL_EMOJIS = ['üò±', 'üî•', '‚ò†Ô∏è', 'üí•', 'ü§Ø', 'üò∂', 'üò≠', 'üò°', 'üíÄ', '‚ö†Ô∏è']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract comprehensive text-based features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üî§ FEATURE ENGINEERING - TEXT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic length features\n",
    "    df[\"title_length\"] = df[\"title\"].str.len()\n",
    "    df[\"desc_length\"] = df[\"description\"].str.len()\n",
    "    df[\"title_word_count\"] = df[\"title\"].str.split().str.len().fillna(0)\n",
    "    df[\"desc_word_count\"] = df[\"description\"].str.split().str.len().fillna(0)\n",
    "    \n",
    "    # Title style features\n",
    "    df[\"caps_ratio\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1)\n",
    "    )\n",
    "    df[\"title_caps_words\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for w in str(x).split() if w.isupper() and len(w) > 1)\n",
    "    )\n",
    "    \n",
    "    # Punctuation features\n",
    "    df[\"question_count\"] = df[\"title\"].str.count(r\"\\?\")\n",
    "    df[\"exclam_count\"] = df[\"title\"].str.count(r\"!\")\n",
    "    df[\"ellipsis_count\"] = df[\"title\"].str.count(r\"\\.\\.\\.\")\n",
    "    df[\"pipe_count\"] = df[\"title\"].str.count(r\"\\|\")\n",
    "    \n",
    "    # Emoji features\n",
    "    df[\"emoji_count\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for c in str(x) if ord(c) > 127462)\n",
    "    )\n",
    "    df[\"emotional_emoji_count\"] = df[\"title\"].apply(\n",
    "        lambda x: sum(1 for e in EMOTIONAL_EMOJIS if e in str(x))\n",
    "    )\n",
    "    \n",
    "    # Clickbait keyword detection\n",
    "    def count_keywords(text, keywords):\n",
    "        text_lower = str(text).lower()\n",
    "        return sum(1 for kw in keywords if kw in text_lower)\n",
    "    \n",
    "    df[\"clickbait_keywords\"] = df[\"title\"].apply(lambda x: count_keywords(x, CLICKBAIT_KEYWORDS))\n",
    "    df[\"piracy_keywords\"] = (\n",
    "        df[\"title\"].apply(lambda x: count_keywords(x, PIRACY_KEYWORDS)) +\n",
    "        df[\"description\"].apply(lambda x: count_keywords(x, PIRACY_KEYWORDS))\n",
    "    )\n",
    "    \n",
    "    # Description quality indicators\n",
    "    df[\"desc_is_empty\"] = (df[\"desc_length\"] < 20).astype(int)\n",
    "    df[\"desc_hashtag_count\"] = df[\"description\"].str.count(r\"#\")\n",
    "    df[\"desc_hashtag_ratio\"] = df[\"desc_hashtag_count\"] / (df[\"desc_word_count\"] + 1)\n",
    "    df[\"desc_has_links\"] = df[\"description\"].str.contains(r\"http|https|www\\.\", regex=True).astype(int)\n",
    "    \n",
    "    # Special patterns\n",
    "    df[\"has_full_movie_claim\"] = df[\"title\"].str.lower().str.contains(\n",
    "        r\"full movie|full hindi movie|full hd movie|complete movie\", regex=True\n",
    "    ).astype(int)\n",
    "    \n",
    "    df[\"has_year_in_title\"] = df[\"title\"].str.contains(r\"\\b20[0-2][0-9]\\b\", regex=True).astype(int)\n",
    "    df[\"has_hd_4k\"] = df[\"title\"].str.lower().str.contains(r\"\\bhd\\b|\\b4k\\b|\\b1080p\\b\", regex=True).astype(int)\n",
    "    \n",
    "    print(\"‚úÖ Text features extracted successfully!\")\n",
    "    return df\n",
    "\n",
    "df = extract_text_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_engagement_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract engagement and metadata features.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìà FEATURE ENGINEERING - ENGAGEMENT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Engagement ratios\n",
    "    df[\"likes_view_ratio\"] = df[\"likes\"] / (df[\"views\"] + 1)\n",
    "    df[\"likes_per_minute\"] = df[\"likes\"] / (df[\"duration_min\"] + 0.1)\n",
    "    df[\"views_per_minute\"] = df[\"views\"] / (df[\"duration_min\"] + 0.1)\n",
    "    \n",
    "    # Log-transformed features\n",
    "    df[\"log_views\"] = np.log1p(df[\"views\"])\n",
    "    df[\"log_likes\"] = np.log1p(df[\"likes\"])\n",
    "    df[\"log_duration\"] = np.log1p(df[\"duration_min\"])\n",
    "    \n",
    "    # Duration-based features\n",
    "    df[\"is_short_video\"] = (df[\"duration_min\"] < 1).astype(int)\n",
    "    df[\"is_very_long\"] = (df[\"duration_min\"] > 60).astype(int)\n",
    "    df[\"duration_mismatch\"] = (\n",
    "        (df[\"has_full_movie_claim\"] == 1) & (df[\"duration_min\"] < 60)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Anomaly detection features\n",
    "    df[\"engagement_score\"] = (\n",
    "        df[\"likes_view_ratio\"] * 100 + \n",
    "        np.log1p(df[\"views\"]) / 10\n",
    "    )\n",
    "    \n",
    "    # Low engagement flag\n",
    "    df[\"low_engagement\"] = (\n",
    "        (df[\"likes_view_ratio\"] < 0.001) & (df[\"views\"] > 10000)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(\"‚úÖ Engagement features extracted successfully!\")\n",
    "    return df\n",
    "\n",
    "df = extract_engagement_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Build Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df: pd.DataFrame, max_features: int = 5000) -> Tuple:\n",
    "    \"\"\"Build the complete feature matrix.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üî® BUILDING FEATURE MATRIX\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Clean text for vectorization\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    df[\"text_clean\"] = df[\"text\"].apply(clean_text)\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=3,\n",
    "        max_df=0.95,\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    X_text = tfidf.fit_transform(df[\"text_clean\"])\n",
    "    print(f\"TF-IDF features: {X_text.shape[1]}\")\n",
    "    \n",
    "    # Category encoding\n",
    "    cat_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "    X_cat = cat_encoder.fit_transform(df[[\"category\"]])\n",
    "    print(f\"Categories: {df['category'].nunique()}\")\n",
    "    \n",
    "    # Numerical features\n",
    "    num_features = [\n",
    "        \"duration_min\", \"views\", \"likes\", \"thumbnail_text_valid\",\n",
    "        \"title_length\", \"desc_length\", \"title_word_count\", \"desc_word_count\",\n",
    "        \"caps_ratio\", \"title_caps_words\",\n",
    "        \"question_count\", \"exclam_count\", \"ellipsis_count\", \"pipe_count\",\n",
    "        \"emoji_count\", \"emotional_emoji_count\",\n",
    "        \"clickbait_keywords\", \"piracy_keywords\",\n",
    "        \"desc_is_empty\", \"desc_hashtag_count\", \"desc_hashtag_ratio\", \"desc_has_links\",\n",
    "        \"has_full_movie_claim\", \"has_year_in_title\", \"has_hd_4k\",\n",
    "        \"likes_view_ratio\", \"likes_per_minute\", \"views_per_minute\",\n",
    "        \"log_views\", \"log_likes\", \"log_duration\",\n",
    "        \"is_short_video\", \"is_very_long\", \"duration_mismatch\",\n",
    "        \"engagement_score\", \"low_engagement\"\n",
    "    ]\n",
    "    \n",
    "    X_num = df[num_features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = scaler.fit_transform(X_num)\n",
    "    \n",
    "    # Combine all features\n",
    "    X = hstack([X_text, csr_matrix(X_num_scaled), csr_matrix(X_cat)])\n",
    "    y = df[\"label\"].values\n",
    "    \n",
    "    print(f\"\\nüìê Final feature matrix shape: {X.shape}\")\n",
    "    print(f\"  - Text features: {X_text.shape[1]}\")\n",
    "    print(f\"  - Numerical features: {len(num_features)}\")\n",
    "    print(f\"  - Category features: {X_cat.shape[1]}\")\n",
    "    \n",
    "    return X, y, tfidf, scaler, cat_encoder, num_features\n",
    "\n",
    "X, y, tfidf, scaler, cat_encoder, num_features = build_feature_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X, y) -> Dict:\n",
    "    \"\"\"Train LightGBM model.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üèÜ TRAINING LIGHTGBM MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Split data: 70% train, 15% validation, 15% test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Train LightGBM\n",
    "    lgb_model = LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüîÑ Training in progress...\")\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation metrics\n",
    "    y_pred_val = lgb_model.predict(X_val)\n",
    "    y_prob_val = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    val_f1 = f1_score(y_val, y_pred_val)\n",
    "    val_auc = roc_auc_score(y_val, y_prob_val)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validation Results:\")\n",
    "    print(f\"   F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"   ROC-AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": lgb_model,\n",
    "        \"X_train\": X_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"val_auc\": val_auc\n",
    "    }\n",
    "\n",
    "results = train_lightgbm(X, y)\n",
    "model = results[\"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_test = results[\"X_test\"]\n",
    "y_test = results[\"y_test\"]\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_prob_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nüî≤ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=[\"Non-Clickbait\", \"Clickbait\"]))\n",
    "\n",
    "print(f\"üéØ ROC-AUC Score: {roc_auc_score(y_test, y_prob_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéöÔ∏è Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(model, X_test, y_test):\n",
    "    \"\"\"Find optimal classification threshold.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéöÔ∏è THRESHOLD OPTIMIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    \n",
    "    # Calculate F1 for each threshold\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "    \n",
    "    print(f\"Default threshold (0.5) F1: {f1_score(y_test, (y_prob > 0.5).astype(int)):.4f}\")\n",
    "    print(f\"Optimal threshold ({optimal_threshold:.3f}) F1: {f1_scores[optimal_idx]:.4f}\")\n",
    "    \n",
    "    # Apply optimal threshold\n",
    "    y_pred_optimal = (y_prob > optimal_threshold).astype(int)\n",
    "    \n",
    "    print(\"\\nüìä Results with Optimal Threshold:\")\n",
    "    print(classification_report(y_test, y_pred_optimal, target_names=[\"Non-Clickbait\", \"Clickbait\"]))\n",
    "    \n",
    "    return optimal_threshold\n",
    "\n",
    "optimal_threshold = optimize_threshold(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importance(model, num_features, tfidf, top_n=20):\n",
    "    \"\"\"Display top feature importances.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä TOP FEATURE IMPORTANCES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    tfidf_features = list(tfidf.get_feature_names_out())\n",
    "    all_features = tfidf_features + num_features\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    print(f\"\\nüîù Top {top_n} Most Important Features:\")\n",
    "    for i, idx in enumerate(indices):\n",
    "        if idx < len(all_features):\n",
    "            print(f\"{i+1:2d}. {all_features[idx]:40s} : {importances[idx]:.4f}\")\n",
    "\n",
    "show_feature_importance(model, num_features, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tfidf, scaler, cat_encoder, num_features, output_dir=\".\"):\n",
    "    \"\"\"Save model and preprocessors for deployment.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üíæ SAVING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    joblib.dump(model, f\"{output_dir}/clickbait_model.joblib\")\n",
    "    joblib.dump(tfidf, f\"{output_dir}/tfidf_vectorizer.joblib\")\n",
    "    joblib.dump(scaler, f\"{output_dir}/scaler.joblib\")\n",
    "    joblib.dump(cat_encoder, f\"{output_dir}/cat_encoder.joblib\")\n",
    "    joblib.dump(num_features, f\"{output_dir}/num_features.joblib\")\n",
    "    \n",
    "    print(f\"‚úÖ Model saved to: {output_dir}/clickbait_model.joblib\")\n",
    "    print(\"‚úÖ All preprocessors saved successfully!\")\n",
    "\n",
    "save_model(model, tfidf, scaler, cat_encoder, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download saved model files (Colab)\n",
    "from google.colab import files\n",
    "\n",
    "files.download('clickbait_model.joblib')\n",
    "files.download('tfidf_vectorizer.joblib')\n",
    "files.download('scaler.joblib')\n",
    "files.download('cat_encoder.joblib')\n",
    "files.download('num_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÆ Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clickbait(title, description, thumbnail_text, category, \n",
    "                      duration_min, views, likes, model_path=\".\"):\n",
    "    \"\"\"\n",
    "    Predict if a video is clickbait.\n",
    "    \n",
    "    Returns:\n",
    "        probability (float): Probability of being clickbait (0-1)\n",
    "        prediction (int): 0 = Not Clickbait, 1 = Clickbait\n",
    "    \"\"\"\n",
    "    # Load model and preprocessors\n",
    "    loaded_model = joblib.load(f\"{model_path}/clickbait_model.joblib\")\n",
    "    loaded_tfidf = joblib.load(f\"{model_path}/tfidf_vectorizer.joblib\")\n",
    "    loaded_scaler = joblib.load(f\"{model_path}/scaler.joblib\")\n",
    "    loaded_cat_encoder = joblib.load(f\"{model_path}/cat_encoder.joblib\")\n",
    "    loaded_num_features = joblib.load(f\"{model_path}/num_features.joblib\")\n",
    "    \n",
    "    # Create dataframe for prediction\n",
    "    df = pd.DataFrame([{\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "        \"thumbnail_text_cleaned\": thumbnail_text,\n",
    "        \"category\": category,\n",
    "        \"duration_min\": duration_min,\n",
    "        \"views\": views,\n",
    "        \"likes\": likes,\n",
    "        \"thumbnail_text_valid\": 1 if thumbnail_text else 0\n",
    "    }])\n",
    "    \n",
    "    # Extract features\n",
    "    df = extract_text_features(df)\n",
    "    df = extract_engagement_features(df)\n",
    "    \n",
    "    # Vectorize text\n",
    "    df[\"text\"] = df[\"title\"] + \" \" + df[\"description\"] + \" \" + df[\"thumbnail_text_cleaned\"]\n",
    "    X_text = loaded_tfidf.transform(df[\"text\"])\n",
    "    \n",
    "    # Get features\n",
    "    X_num = loaded_scaler.transform(df[loaded_num_features].values)\n",
    "    X_cat = loaded_cat_encoder.transform(df[[\"category\"]])\n",
    "    \n",
    "    # Combine and predict\n",
    "    X = hstack([X_text, csr_matrix(X_num), csr_matrix(X_cat)])\n",
    "    prob = loaded_model.predict_proba(X)[0, 1]\n",
    "    pred = int(prob > 0.5)\n",
    "    \n",
    "    return prob, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "prob, pred = predict_clickbait(\n",
    "    title=\"SHOCKING! You Won't Believe What Happened Next üò±üî•\",\n",
    "    description=\"Watch till end for secret reveal!\",\n",
    "    thumbnail_text=\"SHOCKING SECRET\",\n",
    "    category=\"Entertainment\",\n",
    "    duration_min=5.5,\n",
    "    views=100000,\n",
    "    likes=500\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Prediction Results:\")\n",
    "print(f\"   Probability: {prob:.2%}\")\n",
    "print(f\"   Verdict: {'üö® CLICKBAIT' if pred == 1 else '‚úÖ NOT CLICKBAIT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüèÜ Model: LightGBM\")\n",
    "print(f\"üìä Validation F1 Score: {results['val_f1']:.4f}\")\n",
    "print(f\"üìä Validation ROC-AUC: {results['val_auc']:.4f}\")\n",
    "print(f\"üéöÔ∏è Optimal Threshold: {optimal_threshold:.3f}\")\n",
    "print(\"\\n‚úÖ Model saved and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
